library(readr)
library(data.table)
library(FactoMineR)
options(scipen=999)
### Import csv - Sirus et FIchier après scraping
sirus <- fread("C:/Users/maxim/Documents/Hackathon_Insee.git/données/sirus_2017.csv")
scraping <- read.csv2("C:/Users/maxim/Documents/Hackathon_Insee.git/données/scrapping_1000 lignes.csv",
numerals="no.loss", stringsAsFactors = F)
scraping$siret<-str_pad(scraping$siret, 14, side = c("left"), pad = "0")
sirus<-subset(sirus, select = c("sirus_id", "nic", "ape", "eff_3112_et",
"enseigne_et1", "nom_comm_et","cj", "sigle",
"denom", "denom_condense", "x", "y"))
sirus$sirus_id<-str_pad(sirus$sirus_id, 9, side = c("left"), pad = "0")
sirus$nic<-str_pad(sirus$nic, 5, side = c("left"), pad = "0")
sirus$siret<-paste0(sirus$sirus_id,sirus$nic)
scraping_merge <- merge(scraping, sirus, by.x = "siret", by.y = "siret", all.x = T, all.y = F)
#?subset
#scraping$siret<- as.character(scraping$siret)
### Nettoyage texte
nettoyage <- function(texte) {
documents <- Corpus(VectorSource(texte))
# Suppression Accents
accent <- function(x) stri_trans_general(x, "Latin-ASCII") # cela signifie qu'on remplace un caractère encodé en Latin1 par son équivalent le plus proche en ASCII, il n'y a par exemple pas de caractères accentués en ASCII
documents <- tm_map(documents, content_transformer(accent))
# On garde chiffres et lettres seulement
documents <- tm_map(documents, content_transformer(gsub), pattern = "[^a-zA-Z0-9]", replacement = " ")
# Passage minuscule
documents <- tm_map(documents, content_transformer(tolower))
documents <- tm_map(documents, stripWhitespace) #n'enleve pas le tout premier espace
documents <- tm_map(documents, content_transformer(gsub), pattern = "^\\s+", replacement = "")
return(documents$content)
}
nettoyage(scraping_merge$rs_rp)
### Calcul distance - Jaro Winkler
### Variables à prendre en compte pour potentiel matching
# denom
# denom_condense
# enseigne_et1
# nom_comm_et
# sigle
calcul_matching_jw <- function(table) {
# Nettoyage
table_travail <- table
table_travail$nettoyeA <- nettoyage(table_travail$rs_rp)
table_travail$nettoyeB1 <- nettoyage(table_travail$denom)
table_travail$nettoyeB2<- nettoyage(table_travail$denom_condense)
table_travail$nettoyeB3<- nettoyage(table_travail$enseigne_et1)
table_travail$nettoyeB4<- nettoyage(table_travail$nom_comm_et)
table_travail$nettoyeB5 <- nettoyage(table_travail$sigle)
table_travail$dist1 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB1, method = "jw")
table_travail$dist2 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB2, method = "jw")
table_travail$dist3 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB3, method = "jw")
table_travail$dist4 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB4, method = "jw")
table_travail$dist5 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB5, method = "jw")
table_travail[is.na(table_travail)] <- 1
distance_meilleure <- with(table_travail, pmin(dist1, dist2, dist3, dist4))
return(distance_meilleure)
}
scraping_merge$meilleure_dist_jw<- calcul_matching_jw(scraping_merge)
## Calcul distance géographique
scraping_merge$x_rpnum<-as.numeric(scraping$x_rp)
scraping_merge$y_rpnum<-as.numeric(scraping$y_rp)
scraping_merge$distance_geo <- sqrt((scraping_merge$x_rpnum-scraping_merge$x)^2 + (scraping_merge$y_rpnum-scraping_merge$y)^2)
View(scraping_merge)
View(scraping_merge)
scraping_merge$x_rpnum<-as.numeric(scraping_merge$x_rp)
scraping_merge$y_rpnum<-as.numeric(scraping_merge$y_rp)
scraping_merge$distance_geo <- sqrt((scraping_merge$x_rpnum-scraping_merge$x)^2 + (scraping_merge$y_rpnum-scraping_merge$y)^2)
View(scraping_merge)
View(scraping_merge)
table(scraping_merge$is_equals, scraping_merge$distance_geo)
table(scraping_merge$distance_geo, scraping_merge$is_equals)
table(scraping_merge$meilleure_dist_jw, scraping_merge$is_equals)
write.csv(table_qualite, "D:/table_qualite.Csv")
res.pca = PCA(table_qualite, scale.unit=TRUE,              quali.sup=4, ncp=2, graph=T)
View(scraping_merge)
View(scraping_merge)
table_qualite<-subset(scraping_merge, select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,              quali.sup=4, ncp=2, graph=T)
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=13)
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
View(scraping_merge)
View(scraping_merge)
library(corrplot)
install.packages("correplot")
install.packages("corrplot")
cor(table_qualite, method="Pearson")
cor(table_qualite, method="pearson")
?cor
table_qualite_num <- as.numeric(table_qualite)
table_qualite_num <-table_qualite
table_qualite_num <-table_qualite
table_qualite_num$is_equals<-factor(table_qualite_num, labels=c("0", "1"))
table_qualite_num$is_equals<-factor(table_qualite_num$is_equals, labels=c("0", "1"))
View(table_qualite)
View(table_qualite)
cor(table_qualite[1:3,], method="pearson")
cor(table_qualite[,1:3], method="pearson")
cor(table_qualite[,1:3], method="pearson", use=complet.obs)
cor(table_qualite[,1:3], method="pearson", use="complete.obs")
table_qualite<-subset(scraping_merge, select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,              quali.sup=4, ncp=2, graph=T)
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
dimdesc(res.pca, axes=c(1,2))
res.pca = PCA(table_qualite, scale.unit=TRUE,              quali.sup=4, ncp=2, graph=T)
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
dimdesc(res.pca, axes=c(1,2))
resultat_CAH = HCPC(resultat_acm)
resultat_CAH = HCPC(res.pca)
View(table_qualite)
View(table_qualite)
table_qualite<-subset(scraping_merge, distance_geo>0,
select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,              quali.sup=4, ncp=2, graph=T)
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
res.pca = PCA(table_qualite, scale.unit=TRUE,              quali.sup=4, ncp=2, graph=T)
dimdesc(res.pca, axes=c(1,2))
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
res.pca$var
table_qualite<-subset(scraping_merge,
select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,quali.sup=4, ncp=2, graph=T)
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
res.pca$ind
res.pca$ind$coord
res.pca$ind$coord[dim.1]
res.pca$ind$coord["dim.1"]
res.pca$ind$coord
res.pca$ind$coord["Dim.1"]
res.pca$ind$coord["Dim.1",]
res.pca$ind$coord[,"Dim.1"]
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
View(scraping_merge)
?hist
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$Dim.1), col = "green", main = "Score synthétique
en fonction de match ou non")
projectionT$Dim.1
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1)
?duplicated
?unique
write.csv(scraping_merge, "C:/Users/maxim/Documents/Hackathon_Insee.git/données/table_qualite_comp.csv")
?unique
duplicated(scraping_merge, by= c("cabbi","siret"))
anyDuplicated(scraping_merge, by= c("cabbi","siret"))
?sort
?dplyr
install.packages("dplyr")
install.packages("dplyr")
trie <- distinct(scraping_merge2, "scraping_merge2$cabbi","scraping_merge2$siret")
library(dplyr)
trie <- distinct(scraping_merge2, "scraping_merge2$cabbi","scraping_merge2$siret")
library(dplyr)
update.packages("tibble")
library(dplyr)
install.packages("tibble")
library(stringdist)
library(stringi)
library(tm)
library(stringr)
library(readr)
library(data.table)
library(FactoMineR)
library(dplyr)
options(scipen=999)
library(stringdist)
library(stringi)
library(tm)
library(stringr)
library(readr)
library(data.table)
library(FactoMineR)
library(dplyr)
options(scipen=999)
scraping <- read.csv2("C:/Users/maxim/Documents/Hackathon_Insee.git/données/scrapping_1000 lignes.csv",
numerals="no.loss", stringsAsFactors = F)
scraping <- read.csv2("C:/Users/maxim/Documents/Hackathon_Insee.git/données/no doublon_final.csv",
numerals="no.loss", stringsAsFactors = F)
scraping$siret<-str_pad(scraping$siret, 14, side = c("left"), pad = "0")
scraping_merge <- merge(scraping, sirus, by.x = "siret", by.y = "siret", all.x = T, all.y = F)
View(scraping_merge)
View(scraping_merge)
### Nettoyage texte
nettoyage <- function(texte) {
documents <- Corpus(VectorSource(texte))
# Suppression Accents
accent <- function(x) stri_trans_general(x, "Latin-ASCII") # cela signifie qu'on remplace un caractère encodé en Latin1 par son équivalent le plus proche en ASCII, il n'y a par exemple pas de caractères accentués en ASCII
documents <- tm_map(documents, content_transformer(accent))
# On garde chiffres et lettres seulement
documents <- tm_map(documents, content_transformer(gsub), pattern = "[^a-zA-Z0-9]", replacement = " ")
# Passage minuscule
documents <- tm_map(documents, content_transformer(tolower))
documents <- tm_map(documents, stripWhitespace) #n'enleve pas le tout premier espace
documents <- tm_map(documents, content_transformer(gsub), pattern = "^\\s+", replacement = "")
return(documents$content)
}
?stringdist
calcul_matching_jw <- function(table) {
# Nettoyage
table_travail <- table
table_travail$nettoyeA <- nettoyage(table_travail$rs_rp)
table_travail$nettoyeB1 <- nettoyage(table_travail$denom)
table_travail$nettoyeB2<- nettoyage(table_travail$denom_condense)
table_travail$nettoyeB3<- nettoyage(table_travail$enseigne_et1)
table_travail$nettoyeB4<- nettoyage(table_travail$nom_comm_et)
table_travail$nettoyeB5 <- nettoyage(table_travail$sigle)
table_travail$dist1 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB1, method = "jw")
table_travail$dist2 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB2, method = "jw")
table_travail$dist3 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB3, method = "jw")
table_travail$dist4 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB4, method = "jw")
table_travail$dist5 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB5, method = "jw")
table_travail[is.na(table_travail)] <- 1
distance_meilleure <- with(table_travail, pmin(dist1, dist2, dist3, dist4))
return(distance_meilleure)
}
scraping_merge$meilleure_dist_jw<- calcul_matching_jw(scraping_merge)
scraping_merge$x_rpnum<-as.numeric(scraping_merge$x_rp)
scraping_merge$y_rpnum<-as.numeric(scraping_merge$y_rp)
scraping_merge$distance_geo <- sqrt((scraping_merge$x_rpnum-scraping_merge$x)^2 + (scraping_merge$y_rpnum-scraping_merge$y)^2)
table(scraping_merge$distance_geo, scraping_merge$is_equals)
table(scraping_merge$meilleure_dist_jw, scraping_merge$is_equals)
table(scraping_merge$distance_geo, scraping_merge$is_equals)
View(scraping_merge)
table_qualite<-subset(scraping_merge,
select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,quali.sup=4, ncp=2, graph=T)
res.pca$ind
dimdesc(res.pca, axes=c(1,2))
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
table(scraping_merge$is_equals)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1)
View(scraping_merge)
View(scraping_merge)
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
write.csv(scraping_merge, "E:/eval/table_qualite.csv")
scraping$siret<-str_pad(scraping$siret, 14, side = c("left"), pad = "0")
scraping <- read.csv2("C:/Users/maxim/Documents/Hackathon_Insee.git/données/no doublon_final.csv",
numerals="no.loss", stringsAsFactors = F)
scraping$siret<-str_pad(scraping$siret, 14, side = c("left"), pad = "0")
scraping_merge <- merge(scraping, sirus, by.x = "siret", by.y = "siret", all.x = T, all.y = F)
nettoyage <- function(texte) {
documents <- Corpus(VectorSource(texte))
# Suppression Accents
accent <- function(x) stri_trans_general(x, "Latin-ASCII") # cela signifie qu'on remplace un caractère encodé en Latin1 par son équivalent le plus proche en ASCII, il n'y a par exemple pas de caractères accentués en ASCII
documents <- tm_map(documents, content_transformer(accent))
# On garde chiffres et lettres seulement
documents <- tm_map(documents, content_transformer(gsub), pattern = "[^a-zA-Z0-9]", replacement = " ")
# Passage minuscule
documents <- tm_map(documents, content_transformer(tolower))
documents <- tm_map(documents, stripWhitespace) #n'enleve pas le tout premier espace
documents <- tm_map(documents, content_transformer(gsub), pattern = "^\\s+", replacement = "")
return(documents$content)
}
nettoyage(scraping_merge$rs_rp
### Calcul distance - Jaro Winkler
### Variables à prendre en compte pour potentiel matching
# denom
# denom_condense
# enseigne_et1
# nom_comm_et
# sigle
calcul_matching_jw <- function(table) {
# Nettoyage
table_travail <- table
table_travail$nettoyeA <- nettoyage(table_travail$rs_rp)
table_travail$nettoyeB1 <- nettoyage(table_travail$denom)
table_travail$nettoyeB2<- nettoyage(table_travail$denom_condense)
table_travail$nettoyeB3<- nettoyage(table_travail$enseigne_et1)
table_travail$nettoyeB4<- nettoyage(table_travail$nom_comm_et)
table_travail$nettoyeB5 <- nettoyage(table_travail$sigle)
table_travail$dist1 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB1, method = "jw")
table_travail$dist2 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB2, method = "jw")
table_travail$dist3 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB3, method = "jw")
table_travail$dist4 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB4, method = "jw")
table_travail$dist5 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB5, method = "jw")
table_travail[is.na(table_travail)] <- 1
distance_meilleure <- with(table_travail, pmin(dist1, dist2, dist3, dist4))
return(distance_meilleure)
}
table(scraping_merge$indicateur_synthetique)
scraping_merge$meilleure_dist_jw<- calcul_matching_jw(scraping_merge)
scraping_merge$x_rpnum<-as.numeric(scraping_merge$x_rp)
scraping_merge$y_rpnum<-as.numeric(scraping_merge$y_rp)
scraping_merge$distance_geo <- sqrt((scraping_merge$x_rpnum-scraping_merge$x)^2 + (scraping_merge$y_rpnum-scraping_merge$y)^2)
table(scraping_merge$distance_geo, scraping_merge$is_equals)
table(scraping_merge$meilleure_dist_jw, scraping_merge$is_equals)
### ACP
table_qualite<-subset(scraping_merge,
select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,quali.sup=4, ncp=2, graph=T)
res.pca$ind
dimdesc(res.pca, axes=c(1,2))
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
table(scraping_merge$indic_synthetique, scraping_merge$is_equals)
max.print
write.csv(scraping_merge, "E:/eval/table_qualite.csv")
?round
temp$indic_arrondi <- round(temp$indic_synthetique, digits =1)
temp<-scraping_merge
temp$indic_arrondi <- round(temp$indic_synthetique, digits =1)
table(temp$indic_arrondi, temp$is_equals)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1)
?density
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=3)
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1, lwd = 3)
write.csv(scraping_merge, "E:/eval/table_qualite.csv")
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=3)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 3)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=5)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 5)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,lwd = 5)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=8)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 8)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,lwd = 5)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique", lwd=8)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 8)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,lwd = 5)
table_qualite<-subset(scraping_merge,
select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,quali.sup=4, ncp=2, graph=T)
res.pca$ind
dimdesc(res.pca, axes=c(1,2))
install.packages("swirl")
swirl()
library(swirl)
swirl()
5+6
5+7
x -5+7
x <- 5+7
x
y <- x-3
y
z <- c(1.1, 9,
| 3.14)
z <- c(1.1, 9,3.14)
?c
z
c(z,555,z)
z*2+1000
z*2+100
my_sqrt <- sqrt(z) -1
my_sqrt <- sqrt(z-1)
my_sqrt
my_div <- z/my_sqrt
my_div
c(1,2,3,4)+c(1,10)
c(1,2,3,4)+c(0,10)
c(1, 2, 3, 4) + c(0, 10, 100)
z*2+1000
my_div
?solve
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification/2018")
public2018 <- read.csv("ghs_pub.csv", sep = ";", stringsAsFactors = FALSE, dec = ",")
View(public2018)
View(public2018)
table(public2018$CMD)
View(public2018)
View(public2018)
table(public2018$Gravite)
public2018$Gravite <- as.character(substr(public2018$GHM.NRO, 6, 6))
public2018$CMD <- as.character(substr(public2018$GHM.NRO, 1, 2))
table(public2018$CMD)
table(public2018$Gravite)
View(public2018)
View(public2018)
table(public2018$DCS.MCO)
## Pour le moment on garde gravité 1 à 4 / J / T / Autre (renommé en X)
public2018$Gravite[!(public2018$Gravite %in% c("1", "2", "3", "4", "J", "T"))] = "X"
table(public2018$Gravite)
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(logGHSPrix, fill =  public2018$Gravite)) + geom_hist(alpha = 0.2) + ggtitle("Logarithme du prix du séjour selon sa gravité")
## 19 novembre 2018
# Poursuite stats descriptives et modélisation basique tarification pour données projet stat app
#### Champ : données les plus récentes (structurel), séjours dans le public
library(ggplot2)
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(logGHSPrix, fill =  public2018$Gravite)) + geom_hist(alpha = 0.2) + ggtitle("Logarithme du prix du séjour selon sa gravité")
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(logGHSPrix, fill =  public2018$Gravite)) + geom_density(alpha = 0.2) + ggtitle("Logarithme du prix du séjour selon sa gravité")
# Création des variables delog du prix, de catégorie majeure de diagnostic et de gravité.
# Ces variables seront les principaux régresseurs dans l'analyse du prix du séjour.
public2018$logGHSPrix <- log(public2018$GHS.PRI)
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(logGHSPrix, fill =  public2018$Gravite)) + geom_density(alpha = 0.2) + ggtitle("Logarithme du prix du séjour selon sa gravité")
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(SEU.HAU, fill =  public2018$Gravite)) + geom_histogram(alpha = 0.2) + ggtitle("Logarithme du prix du séjour selon sa gravité")
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(SEU.HAU) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil haut")
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(SEU.HAU)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil haut")
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(SEU.HAU)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil haut")
ggplot(public2018, aes(SEU.HAU, fill =  public2018$Gravite)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil haut selon la gravité")
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(SEU.BAS)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil bas")
ggplot(public2018, aes(SEU.BAS, fill =  public2018$Gravite)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil bas selon la gravité")
ggplot(public2018, aes(SEU.HAU)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil haut")
ggplot(public2018, aes(SEU.HAU, fill =  public2018$Gravite)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil haut selon la gravité")
## Distribution des variables SEU.BAS et SEU.HAU ?
ggplot(public2018, aes(SEU.BAS)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil bas")
ggplot(public2018, aes(SEU.BAS, fill =  public2018$Gravite)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil bas selon la gravité")
ggplot(public2018, aes(SEU.BAS, fill =  public2018$Gravite)) + geom_histogram() + ggtitle("Distribution de la variable seuil bas selon la gravité")
ggplot(public2018, aes(SEU.BAS, fill =  public2018$Gravite)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil bas selon la gravité")
ggplot(public2018, aes(SEU.HAU, fill =  public2018$Gravite)) + geom_histogram() + ggtitle("Distribution de la variable seuil haut selon la gravité")
lm(public2018$GHS.PRI~ public2018$CMD + public2018$DCS.MCO + public2018$Gravite)
modele_base1 <- lm(public2018$GHS.PRI~ public2018$CMD + public2018$DCS.MCO + public2018$Gravite)
summary(modele_base1)
# Avec le log du prix
modele_base1 <- lm(public2018$logGHSPrix~ public2018$CMD + public2018$DCS.MCO + public2018$Gravite)
# Avec le prix
modele_base1 <- lm(public2018$GHS.PRI~ public2018$CMD + public2018$DCS.MCO + public2018$Gravite)
summary(modele_base1)
# NB : Obstrétrique / DCS = "O" (médecine pour grossesse et accouchement) = CMD 14
# Avec le log du prix
modele_base2 <- lm(public2018$logGHSPrix~ public2018$CMD + public2018$DCS.MCO + public2018$Gravite)
summary(modele_base2)
ggplot(public2018, aes(SEU.HAU, fill =  public2018$Gravite)) + geom_histogram(alpha = 0.2) + ggtitle("Distribution de la variable seuil haut selon la gravité")
# Avec le log du prix
modele_base2 <- lm(public2018$logGHSPrix~ public2018$CMD + public2018$DCS.MCO + public2018$Gravite)
modele_base3 <- lm(public2018$GHS.PRI~ public2018$CMD +
public2018$DCS.MCO + public2018$Gravite + public2018$SEU.HAU)
summary(modele_base3)
## Analyse résidus pour ce modèle.
modele_base2.res = resid(modele_base2)
summary(modele_base2.res)
modele_base2.res = resid(modele_base2)
summary(modele_base2.res)
plot(public2018$GHS.PRI, modele_base2.res,
ylab="Résidus", xlab="Prix du séjour",
main="Tarification des séjours hospitaliers - Résidus")
abline(0, 0)
summary(modele_base2.res)
plot(public2018$logGHSPrix, modele_base2.res,
ylab="Résidus", xlab="Prix du séjour",
main="Tarification des séjours hospitaliers - Résidus")
abline(0, 0)
# Avec le prix
modele_base1 <- lm(public2018$GHS.PRI~ public2018$CMD + public2018$DCS.MCO + public2018$Gravite)
summary(modele_base1)
# NB : Obstrétrique / DCS = "O" (médecine pour grossesse et accouchement) = CMD 14
modele_base1.res = resid(modele_base1)
summary(modele_base1.res)
plot(public2018$GHS.PRI, modele_base1.res,
ylab="Résidus", xlab="Prix du séjour",
main="Tarification des séjours hospitaliers - Résidus")
abline(0, 0)
## Du coup pas mal de droites pour ceux qui ont les mêmes covariables pour un Y différent??
