en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1)
View(scraping_merge)
View(scraping_merge)
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
write.csv(scraping_merge, "E:/eval/table_qualite.csv")
scraping$siret<-str_pad(scraping$siret, 14, side = c("left"), pad = "0")
scraping <- read.csv2("C:/Users/maxim/Documents/Hackathon_Insee.git/données/no doublon_final.csv",
numerals="no.loss", stringsAsFactors = F)
scraping$siret<-str_pad(scraping$siret, 14, side = c("left"), pad = "0")
scraping_merge <- merge(scraping, sirus, by.x = "siret", by.y = "siret", all.x = T, all.y = F)
nettoyage <- function(texte) {
documents <- Corpus(VectorSource(texte))
# Suppression Accents
accent <- function(x) stri_trans_general(x, "Latin-ASCII") # cela signifie qu'on remplace un caractère encodé en Latin1 par son équivalent le plus proche en ASCII, il n'y a par exemple pas de caractères accentués en ASCII
documents <- tm_map(documents, content_transformer(accent))
# On garde chiffres et lettres seulement
documents <- tm_map(documents, content_transformer(gsub), pattern = "[^a-zA-Z0-9]", replacement = " ")
# Passage minuscule
documents <- tm_map(documents, content_transformer(tolower))
documents <- tm_map(documents, stripWhitespace) #n'enleve pas le tout premier espace
documents <- tm_map(documents, content_transformer(gsub), pattern = "^\\s+", replacement = "")
return(documents$content)
}
nettoyage(scraping_merge$rs_rp
### Calcul distance - Jaro Winkler
### Variables à prendre en compte pour potentiel matching
# denom
# denom_condense
# enseigne_et1
# nom_comm_et
# sigle
calcul_matching_jw <- function(table) {
# Nettoyage
table_travail <- table
table_travail$nettoyeA <- nettoyage(table_travail$rs_rp)
table_travail$nettoyeB1 <- nettoyage(table_travail$denom)
table_travail$nettoyeB2<- nettoyage(table_travail$denom_condense)
table_travail$nettoyeB3<- nettoyage(table_travail$enseigne_et1)
table_travail$nettoyeB4<- nettoyage(table_travail$nom_comm_et)
table_travail$nettoyeB5 <- nettoyage(table_travail$sigle)
table_travail$dist1 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB1, method = "jw")
table_travail$dist2 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB2, method = "jw")
table_travail$dist3 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB3, method = "jw")
table_travail$dist4 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB4, method = "jw")
table_travail$dist5 <- stringdist(table_travail$nettoyeA,table_travail$nettoyeB5, method = "jw")
table_travail[is.na(table_travail)] <- 1
distance_meilleure <- with(table_travail, pmin(dist1, dist2, dist3, dist4))
return(distance_meilleure)
}
table(scraping_merge$indicateur_synthetique)
scraping_merge$meilleure_dist_jw<- calcul_matching_jw(scraping_merge)
scraping_merge$x_rpnum<-as.numeric(scraping_merge$x_rp)
scraping_merge$y_rpnum<-as.numeric(scraping_merge$y_rp)
scraping_merge$distance_geo <- sqrt((scraping_merge$x_rpnum-scraping_merge$x)^2 + (scraping_merge$y_rpnum-scraping_merge$y)^2)
table(scraping_merge$distance_geo, scraping_merge$is_equals)
table(scraping_merge$meilleure_dist_jw, scraping_merge$is_equals)
### ACP
table_qualite<-subset(scraping_merge,
select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,quali.sup=4, ncp=2, graph=T)
res.pca$ind
dimdesc(res.pca, axes=c(1,2))
plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=4)
scraping_merge["indic_synthetique"]<-res.pca$ind$coord[,"Dim.1"]
table(scraping_merge$indic_synthetique, scraping_merge$is_equals)
max.print
write.csv(scraping_merge, "E:/eval/table_qualite.csv")
?round
temp$indic_arrondi <- round(temp$indic_synthetique, digits =1)
temp<-scraping_merge
temp$indic_arrondi <- round(temp$indic_synthetique, digits =1)
table(temp$indic_arrondi, temp$is_equals)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non")
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1)
?density
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=3)
lines(density(projectionF$indic_synthetique), col = "red")
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1, lwd = 3)
write.csv(scraping_merge, "E:/eval/table_qualite.csv")
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=3)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 3)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=5)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 5)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,lwd = 5)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique
en fonction de match ou non", lwd=8)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 8)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,lwd = 5)
projectionT <- subset(scraping_merge, scraping_merge$is_equals == "True")
projectionF <- subset(scraping_merge, scraping_merge$is_equals == "False")
plot(density(projectionT$indic_synthetique), col = "green", main = "Score synthétique", lwd=8)
lines(density(projectionF$indic_synthetique), col = "red", lwd = 8)
legend("topright", 95, legend=c("Match", "Non match"),
col=c("green", "red"), lty=1,lwd = 5)
table_qualite<-subset(scraping_merge,
select = c("position_page","meilleure_dist_jw","distance_geo","is_equals"))
res.pca = PCA(table_qualite, scale.unit=TRUE,quali.sup=4, ncp=2, graph=T)
res.pca$ind
dimdesc(res.pca, axes=c(1,2))
install.packages("swirl")
swirl()
library(swirl)
swirl()
5+6
5+7
x -5+7
x <- 5+7
x
y <- x-3
y
z <- c(1.1, 9,
| 3.14)
z <- c(1.1, 9,3.14)
?c
z
c(z,555,z)
z*2+1000
z*2+100
my_sqrt <- sqrt(z) -1
my_sqrt <- sqrt(z-1)
my_sqrt
my_div <- z/my_sqrt
my_div
c(1,2,3,4)+c(1,10)
c(1,2,3,4)+c(0,10)
c(1, 2, 3, 4) + c(0, 10, 100)
z*2+1000
my_div
?solve
# Import donnÃ©es : on se place sur les donnÃ©es 2018, pour les hÃ´pitaux publics Ã  ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/DonnÃ©es tarification/2018")
public2018 <- read.csv("ghs_pub.csv", sep = ";", stringsAsFactors = FALSE, dec = ",")
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(explor)
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification")
df <- read.csv("data_compile_scraping_20181130.csv", sep = ",", stringsAsFactors = FALSE,
dec = ".", encoding = "UTF-8")
data_2017 <- subset(df, subset = (annee == 2017))
colnames(data_2017)
##colnames(data_2017)<- c("id", "annee","type","ghm.nro","ghs.nro","cmd.cod",
##                     "dcs.mco","ghm.lib","seu.bas","seu.haut","ghs.prix","exh.pri",
##                    "date.effet","exf.forfait","exb.journalier","effectif","duree.moy",
###                   "duree.sd", "duree.var", "duree.cv",
###                  "duree.min","duree.max","duree.mode", "duree.mediane", "duree.p10","duree.p95",
##                 "deces.nb","deces.pct","age.moy","age.sd","diag.moy","actes.moy","actesCl.moy",
#                "actesNCl.moy")
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
### MCA
summary(data_2017$ghs.pri)
data_2017$prix_tranche = cut(data_2017$ghs.pri,c(0, 1000, 2500, 5000, 100000))
levels(data_2017$prix_tranche) = c("- 1000€","1000 à 2500€","2500 à 5000€","+ 5000€")
table(data_2017$prix_tranche)
## Pour le moment on garde gravité 1 à 4 / J / T / Autre (renommé en X)
data_2017$Gravite <- as.character(substr(data_2017$ghm.nro, 6, 6))
data_2017$Gravite[!(data_2017$Gravite %in% c("1", "2", "3", "4", "J", "T"))] = "X"
data_2017$Gravite<-paste("Gravité :",data_2017$Gravite)
table(data_2017$Gravite)
# Pour les nuitées
summary(data_2017$nuitées.moy)
# Prix en 4 catégories : pour avoir environ un quart des obs à chaque fois
data_2017$nuitees_tranche = cut(data_2017$nuitées.moy,c(-1, 2, 6, 12, 1000))
levels(data_2017$nuitees_tranche) = c("- 2 nuits","2-6 nuits","6-12 nuits","+ 12 nuits")
table(data_2017$nuitees_tranche)
library(factoextra)
library(ggplot2)
library("factoextra", lib.loc="~/R/win-library/3.5")
detach("package:factoextra", unload=TRUE)
library(factoextra)
detach("package:factoextra", unload=TRUE)
library(factoextra)
detach("package:factoextra", unload=TRUE)
library("FactoMineR", lib.loc="~/R/win-library/3.5")
detach("package:FactoMineR", unload=TRUE)
library(FactoMineR)
library(factoextra)
library(explor)
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification")
df <- read.csv("data_compile_scraping_20181130.csv", sep = ",", stringsAsFactors = FALSE,
dec = ".", encoding = "UTF-8")
data_2017 <- subset(df, subset = (annee == 2017))
colnames(data_2017)
##colnames(data_2017)<- c("id", "annee","type","ghm.nro","ghs.nro","cmd.cod",
##                     "dcs.mco","ghm.lib","seu.bas","seu.haut","ghs.prix","exh.pri",
##                    "date.effet","exf.forfait","exb.journalier","effectif","duree.moy",
###                   "duree.sd", "duree.var", "duree.cv",
###                  "duree.min","duree.max","duree.mode", "duree.mediane", "duree.p10","duree.p95",
##                 "deces.nb","deces.pct","age.moy","age.sd","diag.moy","actes.moy","actesCl.moy",
#                "actesNCl.moy")
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
### MCA
summary(data_2017$ghs.pri)
data_2017$prix_tranche = cut(data_2017$ghs.pri,c(0, 1000, 2500, 5000, 100000))
levels(data_2017$prix_tranche) = c("- 1000€","1000 à 2500€","2500 à 5000€","+ 5000€")
table(data_2017$prix_tranche)
## Pour le moment on garde gravité 1 à 4 / J / T / Autre (renommé en X)
data_2017$Gravite <- as.character(substr(data_2017$ghm.nro, 6, 6))
data_2017$Gravite[!(data_2017$Gravite %in% c("1", "2", "3", "4", "J", "T"))] = "X"
data_2017$Gravite<-paste("Gravité :",data_2017$Gravite)
table(data_2017$Gravite)
# Pour les nuitées
summary(data_2017$nuitées.moy)
# Prix en 4 catégories : pour avoir environ un quart des obs à chaque fois
data_2017$nuitees_tranche = cut(data_2017$nuitées.moy,c(-1, 2, 6, 12, 1000))
levels(data_2017$nuitees_tranche) = c("- 2 nuits","2-6 nuits","6-12 nuits","+ 12 nuits")
table(data_2017$nuitees_tranche)
# CMD avec indication de la variable
data_2017$cmd.cod<-paste("CMD :",data_2017$cmd.cod)
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
summary(data_2017$effectif)
### MCA
# On utilise les poids normalisés afin de ne pas avoir de problème pour la CAH...
data_2017<- effectif_norm <- data_2017$effectif/sum(data_2017$effectif)
## 30  novembre 2018 : éléments d'analyse de données pour regarder différentes variables,
# Notamment sur ce qu'il se passe pour les CMD
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(explor)
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification")
df <- read.csv("data_compile_scraping_20181130.csv", sep = ",", stringsAsFactors = FALSE,
dec = ".", encoding = "UTF-8")
data_2017 <- subset(df, subset = (annee == 2017))
colnames(data_2017)
##colnames(data_2017)<- c("id", "annee","type","ghm.nro","ghs.nro","cmd.cod",
##                     "dcs.mco","ghm.lib","seu.bas","seu.haut","ghs.prix","exh.pri",
##                    "date.effet","exf.forfait","exb.journalier","effectif","duree.moy",
###                   "duree.sd", "duree.var", "duree.cv",
###                  "duree.min","duree.max","duree.mode", "duree.mediane", "duree.p10","duree.p95",
##                 "deces.nb","deces.pct","age.moy","age.sd","diag.moy","actes.moy","actesCl.moy",
#                "actesNCl.moy")
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
### MCA
# On utilise les poids normalisés afin de ne pas avoir de problème pour la CAH...
data_2017<- effectif_norm <- data_2017$effectif/sum(data_2017$effectif)
summary(data_2017$ghs.pri)
summary(data_2017$effectif)
data_2017$prix_tranche = cut(data_2017$ghs.pri,c(0, 1000, 2500, 5000, 100000))
levels(data_2017$prix_tranche) = c("- 1000€","1000 à 2500€","2500 à 5000€","+ 5000€")
table(data_2017$prix_tranche)
## Pour le moment on garde gravité 1 à 4 / J / T / Autre (renommé en X)
data_2017$Gravite <- as.character(substr(data_2017$ghm.nro, 6, 6))
data_2017$Gravite[!(data_2017$Gravite %in% c("1", "2", "3", "4", "J", "T"))] = "X"
data_2017$Gravite<-paste("Gra
## 30  novembre 2018 : éléments d'analyse de données pour regarder différentes variables,
# Notamment sur ce qu'il se passe pour les CMD
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(explor)
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification")
df <- read.csv("data_compile_scraping_20181130.csv", sep = ",", stringsAsFactors = FALSE,
dec = ".", encoding = "UTF-8")
data_2017 <- subset(df, subset = (annee == 2017))
colnames(data_2017)
##colnames(data_2017)<- c("id", "annee","type","ghm.nro","ghs.nro","cmd.cod",
##                     "dcs.mco","ghm.lib","seu.bas","seu.haut","ghs.prix","exh.pri",
##                    "date.effet","exf.forfait","exb.journalier","effectif","duree.moy",
###                   "duree.sd", "duree.var", "duree.cv",
###                  "duree.min","duree.max","duree.mode", "duree.mediane", "duree.p10","duree.p95",
##                 "deces.nb","deces.pct","age.moy","age.sd","diag.moy","actes.moy","actesCl.moy",
#                "actesNCl.moy")
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
### MCA
# On utilise les poids normalisés afin de ne pas avoir de problème pour la CAH...
data_2017<- effectif_norm <- data_2017$effectif/sum(data_2017$effectif)
summary(data_2017$ghs.pri)
summary(data_2017$effectif)
data_2017$prix_tranche = cut(data_2017$ghs.pri,c(0, 1000, 2500, 5000, 100000))
levels(data_2017$prix_tranche) = c("- 1000€","1000 à 2500€","2500 à 5000€","+ 5000€")
table(data_2017$prix_tranche)
## Pour le moment on garde gravité 1 à 4 / J / T / Autre (renommé en X)
data_2017$Gravite <- as.character(substr(data_2017$ghm.nro, 6, 6))
data_2017$Gravite[!(data_2017$Gravite %in% c("1", "2", "3", "4", "J", "T"))] = "X"
data_2017$Gravite<-paste("Gravité :",data_2017$Gravite)
table(data_2017$Gravite)
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification")
df <- read.csv("data_compile_scraping_20181130.csv", sep = ",", stringsAsFactors = FALSE,
dec = ".", encoding = "UTF-8")
data_2017 <- subset(df, subset = (annee == 2017))
colnames(data_2017)
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
### MCA
# On utilise les poids normalisés afin de ne pas avoir de problème pour la CAH...
data_2017<- effectif_norm <- data_2017$effectif/sum(data_2017$effectif)
## 30  novembre 2018 : éléments d'analyse de données pour regarder différentes variables,
# Notamment sur ce qu'il se passe pour les CMD
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(explor)
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification")
df <- read.csv("data_compile_scraping_20181130.csv", sep = ",", stringsAsFactors = FALSE,
dec = ".", encoding = "UTF-8")
data_2017 <- subset(df, subset = (annee == 2017))
colnames(data_2017)
##colnames(data_2017)<- c("id", "annee","type","ghm.nro","ghs.nro","cmd.cod",
##                     "dcs.mco","ghm.lib","seu.bas","seu.haut","ghs.prix","exh.pri",
##                    "date.effet","exf.forfait","exb.journalier","effectif","duree.moy",
###                   "duree.sd", "duree.var", "duree.cv",
###                  "duree.min","duree.max","duree.mode", "duree.mediane", "duree.p10","duree.p95",
##                 "deces.nb","deces.pct","age.moy","age.sd","diag.moy","actes.moy","actesCl.moy",
#                "actesNCl.moy")
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
### MCA
# On utilise les poids normalisés afin de ne pas avoir de problème pour la CAH...
data_2017$effectif_norm <- data_2017$effectif/sum(data_2017$effectif)
summary(data_2017$ghs.pri)
summary(data_2017$effectif)
data_2017$prix_tranche = cut(data_2017$ghs.pri,c(0, 1000, 2500, 5000, 100000))
levels(data_2017$prix_tranche) = c("- 1000€","1000 à 2500€","2500 à 5000€","+ 5000€")
table(data_2017$prix_tranche)
## Pour le moment on garde gravité 1 à 4 / J / T / Autre (renommé en X)
data_2017$Gravite <- as.character(substr(data_2017$ghm.nro, 6, 6))
data_2017$Gravite[!(data_2017$Gravite %in% c("1", "2", "3", "4", "J", "T"))] = "X"
data_2017$Gravite<-paste("Gravité :",data_2017$Gravite)
table(data_2017$Gravite)
# Pour les nuitées
summary(data_2017$nuitées.moy)
# Prix en 4 catégories : pour avoir environ un quart des obs à chaque fois
data_2017$nuitees_tranche = cut(data_2017$nuitées.moy,c(-1, 2, 6, 12, 1000))
levels(data_2017$nuitees_tranche) = c("- 2 nuits","2-6 nuits","6-12 nuits","+ 12 nuits")
table(data_2017$nuitees_tranche)
data_2017$cmd.cod<-paste("CMD :",data_2017$cmd.cod)
input_MCA1 <- subset(data_2017, select = c("cmd.cod", "Gravite",
"nuitees_tranche", "prix_tranche"))
input_MCA1$Gravite<- as.factor(input_MCA1$Gravite)
input_MCA1$cmd.cod<- as.factor(input_MCA1$cmd.cod)
mon_ACM1 <- MCA(input_MCA1, row.w = data_2017$effectif_norm, ncp = 5)
CAH1 <- HCPC(mon_ACM1)
mon_ACM1 <- MCA(input_MCA1, row.w = data_2017$effectif, ncp = 5)
explor(mon_ACM1)
CAH1 <- HCPC(mon_ACM1)
mon_ACM1 <- MCA(input_MCA1, ncp = 5)
explor(mon_ACM1)
CAH1 <- HCPC(mon_ACM1)
# Import données : on se place sur les données 2018, pour les hôpitaux publics à ce stade.
setwd("C:/Users/maxim/Documents/ensae-statapp-2019/Data/Données tarification")
df <- read.csv("data_compile_scraping_20181130.csv", sep = ",", stringsAsFactors = FALSE,
dec = ".", encoding = "UTF-8")
data_2017 <- subset(df, subset = (annee == 2017))
colnames(data_2017)
# Suppression NAs (poids à 0 d'aprè légère vérif)
# nrow(data_2017[complete.cases(data_2017[ , "effectif"]),])
data_2017 <- data_2017[complete.cases(data_2017[ , "effectif"]),]
# Varibales avec NAs pour le prix ?!
summary(data_2017$cmd.cod)
## Attention : 20 maladies pour lesquelles on a des infos scrapées mais pas de prix...
# On les écarte pour l'ACM à ce stade.
data_2017 <- data_2017[complete.cases(data_2017[ , "ghs.pri"]),]
# On utilise les poids normalisés afin de ne pas avoir de problème pour la CAH...
data_2017$effectif_norm <- data_2017$effectif/sum(data_2017$effectif)
# Création des variables catégorielles
summary(data_2017$ghs.pri)
summary(data_2017$effectif)
data_2017$prix_tranche = cut(data_2017$ghs.pri,c(0, 1000, 2500, 5000, 100000))
levels(data_2017$prix_tranche) = c("- 1000€","1000 à 2500€","2500 à 5000€","+ 5000€")
table(data_2017$prix_tranche)
## Pour le moment on garde gravité 1 à 4 / J / T / Autre (renommé en X)
data_2017$Gravite <- as.character(substr(data_2017$ghm.nro, 6, 6))
data_2017$Gravite[!(data_2017$Gravite %in% c("1", "2", "3", "4", "J", "T"))] = "X"
data_2017$Gravite<-paste("Gravité :",data_2017$Gravite)
table(data_2017$Gravite)
# Pour les nuitées
summary(data_2017$nuitées.moy)
# Prix en 4 catégories : pour avoir environ un quart des obs à chaque fois
data_2017$nuitees_tranche = cut(data_2017$nuitées.moy,c(-1, 2, 6, 12, 1000))
levels(data_2017$nuitees_tranche) = c("- 2 nuits","2-6 nuits","6-12 nuits","+ 12 nuits")
table(data_2017$nuitees_tranche)
# CMD avec indication de la variable
data_2017$cmd.cod<-paste("CMD :",data_2017$cmd.cod)
# Type hôpital
data_2017$type <- paste("Hôpital :", data_2017$type)
# Âge
summary(data_2017$age.moy)
summary(data_2017$age.moy)
# 4 tranches pour les quartiles comme d'hab
data_2017$age_tranche = cut(data_2017$nuitées.moy,c(-1, 45, 60, 70, 120))
levels(data_2017$age_tranche) = c("-45 ans","45-60 ans","60-70 ans","+ 70 ans")
table(data_2017$age_tranche)
summary(data_2017$age.moy)
# 4 tranches pour les quartiles comme d'hab
data_2017$age_tranche = cut(data_2017$age.moy,c(-1, 45, 60, 70, 120))
levels(data_2017$age_tranche) = c("-45 ans","45-60 ans","60-70 ans","+ 70 ans")
table(data_2017$age_tranche)
input_MCA1 <- subset(data_2017, select = c("cmd.cod", "Gravite",
"nuitees_tranche", "prix_tranche", "type",
"age_tranche"))
input_MCA1$Gravite<- as.factor(input_MCA1$Gravite)
input_MCA1$cmd.cod<- as.factor(input_MCA1$cmd.cod)
input_MCA1$type<- as.factor(input_MCA1$type)
mon_ACM1 <- MCA(input_MCA1, row.w = data_2017$effectif_norm, ncp = 5)
explor(mon_ACM1)
input_MCA1 <- subset(data_2017, select = c("cmd.cod", "Gravite",
"nuitees_tranche", "prix_tranche", "type",
"age_tranche"))
input_MCA1$Gravite<- as.factor(input_MCA1$Gravite)
input_MCA1$cmd.cod<- as.factor(input_MCA1$cmd.cod)
input_MCA1$type<- as.factor(input_MCA1$type)
mon_ACM1 <- MCA(input_MCA1, row.w = data_2017$effectif_norm, ncp = 5)
explor(mon_ACM1)
mon_ACM1 <- MCA(input_MCA1, row.w = data_2017$effectif_norm, ncp = 8)
explor(mon_ACM1)
mon_ACM1 <- MCA(input_MCA1, row.w = data_2017$effectif_norm, ncp = 10)
explor(mon_ACM1)
fviz_mca_var(mon_ACM1)
table(input_MCA1$cmd.cod)
explor(mon_ACM1)
